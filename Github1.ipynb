{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Modelling : TF-IDF, Cosine Similarity, BM-25\n",
    "\n",
    "### TF-IDF\n",
    "Dataset chosen is from a collection of about 7,000 Yelp reviews from the [Yelp Dataset Challenge](https://www.yelp.com/dataset_challenge).\n",
    "Each line corresponds to a review on a particular business. Each review has a unique \"ID\" and the text content is in the \"review\" field.\n",
    "I treat each review as a document. Given a query, you need to calculate its TF-IDF score in each review.\n",
    "\n",
    "`TF = number of times word occurs in a document`\n",
    "\n",
    "`IDF = log(total number of documents / number of documents containing the word)`\n",
    "\n",
    "IDF has several formulations I have chosen the above for simplicity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review': 'every time i go horrible stale taco shells hard uncooked beans need i go on stay clear of this taco bell ', 'id': 'oN9FgM2DYrMjUKdrJS2lYQ'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "data=[]\n",
    "with open(\"review.json\",'r') as json_file:\n",
    "    for line in json_file:\n",
    "        data.append(json.loads(line))\n",
    "N=len(data) #Num of documents in the corpus \n",
    "#Example: First document\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Preprocessing will help us mine tf and df for the entire vocabulary of the document corpus.\n",
    "This will be used in later functions to implement TF-IDF, cosine and BM-25 methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "#Precalculate df of all terms in all docs, later use this to calculate docmag for cosine scoring\n",
    "df_dict=dict()\n",
    "for doc in data:\n",
    "    terms_unique=set(doc['review'].split())\n",
    "    for term in terms_unique:\n",
    "        df_dict[term]=df_dict.get(term,0)+1\n",
    "\n",
    "#Calculate idf for all terms\n",
    "idf_dict=dict()\n",
    "for term in df_dict:\n",
    "    idf_dict[term]=(math.log(N/float(df_dict[term])))\n",
    "avgdl=0 #Average doc length initialise to 0\n",
    "total_len=0 #Add doc lengths here\n",
    "doc_info=dict()\n",
    "\n",
    "#Preprocessing\n",
    "#Go through all docs to get every doc's tf\n",
    "for doc in data:\n",
    "    docid=doc['id']\n",
    "    terms=doc['review'].split()\n",
    "    total_len+=len(terms)\n",
    "    doc_dict=dict()\n",
    "    docmag=0\n",
    "    for term in terms:\n",
    "        if term not in doc_dict:\n",
    "            doc_dict[term]=terms.count(term)\n",
    "    for term,tf in doc_dict.items():\n",
    "        docmag+=(idf_dict[term]*tf)**2.\n",
    "    docmag=math.sqrt(docmag)\n",
    "    doc_info[docid]=(doc_dict,docmag)\n",
    "#Get average doc length across the corpus for BM25\n",
    "avgdl=total_len/N\n",
    "\n",
    "#Following functionality splits testquery and stores doc scores for all docs for the query terms\n",
    "def score_documents(testquery,scoring):\n",
    "    query=testquery.split()\n",
    "    idfarray=[0.] * len(query) #Initialiase idf array with zeros len = no of query terms\n",
    "    doc_metric=dict() #Will contain only docs that have search queries and tf values\n",
    "    #Next part helps calculate docmetric with key=docid and value depending on scoring type\n",
    "    for doc in data:\n",
    "        docid=doc['id'] #Get Doc id\n",
    "        tfarray=[0] * len(query) \n",
    "        tfarray= [doc_info[docid][0].get(x,0) for x in query]\n",
    "        if tfarray!=[0] * len(query): #Only store metric for doc with at least one query term\n",
    "            if scoring==\"tfidf_score\":\n",
    "                doc_metric[docid]=(tfarray) #Store tf array and 0 for compatibility\n",
    "            elif scoring==\"cosine_score\":\n",
    "                doc_metric[docid]=(tfarray,doc_info[docid][1]) #store tuple of tfarray of only search terms and docmag\n",
    "            elif scoring==\"bm25_score\":\n",
    "                doc_metric[docid]=(tfarray,sum(doc_info[docid][0].values())) #store tuple of tf array and doc length\n",
    "    #Calculate IDF of only query terms\n",
    "    idfarray=[idf_dict.get(q,0) for q in query]\n",
    "    return doc_metric,idfarray\n",
    "\n",
    "#Function to calculate tf*idf score\n",
    "def tfidf_score():\n",
    "    score=dict() #Key= Document ID Value=Sum of tf*idf score of each \n",
    "    #Calculate composite Sum of tf*idf score for each term\n",
    "    for x in list(doc_metric): #iterate over all docs with non zero tf\n",
    "        scr=0\n",
    "        for i in range(len(doc_metric[x])):\n",
    "            scr+=doc_metric[x][i]*idfarray[i]\n",
    "        score[x]=scr\n",
    "    return score \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Top 10 documents that have highest score for the query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to print results\n",
    "def printresults():\n",
    "    topten = sorted(composite_score.items(),key = lambda x:x[1], reverse=True)[:10]\n",
    "    print(\"Query=\",\"\\\"\",testquery,\"\\\"\")\n",
    "    print(\"Rank Document ID \\t\\t Score \\n\")\n",
    "    i=1\n",
    "    for x in topten:\n",
    "        print(i,\"{0} \\t {1}\".format(*x,))\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example query and scoring method with top ten documents printed with scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query= \" best bbq \"\n",
      "Rank Document ID \t\t Score \n",
      "\n",
      "1 YbQvHNrjZJ38mnh5rLuq7w \t 26.319774733191345\n",
      "2 P31kXP4oan6ZQm69TN6tIA \t 21.933145610992785\n",
      "3 x5esEK6J9XkA_vbvVbG8Gg \t 19.506448347290707\n",
      "4 mWs26TrBM7ogwCM9UfVJFg \t 17.54651648879423\n",
      "5 NCfX4AxDvQ3QRyXKtmhVwQ \t 17.54651648879423\n",
      "6 e5INq6DAZn2zMHicKQl07Q \t 15.119819225092153\n",
      "7 4WTG1-9mw8YHEyaTu8dQww \t 15.119819225092153\n",
      "8 x3n_l3GhBx78y6jWX4fStg \t 13.719523009475362\n",
      "9 Wp8jYXL1DQrgrnZIFmufFg \t 13.159887366595672\n",
      "10 jrEx93eYKIjCW2nrkwjZpQ \t 13.159887366595672\n"
     ]
    }
   ],
   "source": [
    "testquery=\"best bbq\"\n",
    "scoring=\"tfidf_score\"\n",
    "doc_metric,idfarray=score_documents(testquery,scoring)\n",
    "composite_score=tfidf_score()\n",
    "printresults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking with TF-IDF + Cosine\n",
    "\n",
    "Instead of using the sum of TF-IDF scores, now we still use the TF-IDF scores to weigh each term, but also cosine between the query vector and the document vector to assign a similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_score():\n",
    "    score=dict()\n",
    "    for x in list(doc_metric): #iterate over all docs with non zero tf for at least one search term\n",
    "        scr=0\n",
    "        #Syntax= doc_metric[\"docid\"]=([tfarray],docmag)  doc_metric[x][1]=docmag\n",
    "        y=len(doc_metric[x][0]) #no of terms\n",
    "        for i in range(y):\n",
    "            scr+=(doc_metric[x][0][i]*idfarray[i])*(1./math.sqrt(float(y)))*(1/doc_metric[x][1])\n",
    "        score[x]=scr\n",
    "    #Calculate composite Sum of tf*idf score for each term\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query best bbq\n",
      "Rank Document ID \t\t Score \n",
      "\n",
      "1 6wRJtHhvQsS6vOse466_3w \t 0.5345284475881048\n",
      "2 x5esEK6J9XkA_vbvVbG8Gg \t 0.4360447415638787\n",
      "3 eAXFFP3GxUfGjQlAZDB_CQ \t 0.42255218023429036\n",
      "4 7LaBODDEaUNRpLPDG_bKtQ \t 0.40004536886838066\n",
      "5 P31kXP4oan6ZQm69TN6tIA \t 0.35771827655726385\n",
      "6 ZAn6zB6VOCsJ1OsGRv-NVA \t 0.35459450721620783\n",
      "7 8p-KEtrrTmLv-o1mKpUy1A \t 0.33997398367851134\n",
      "8 RHWT1KVeIw2FT7i5BP_TVw \t 0.31605256633839557\n",
      "9 _fNfowXaxXcYChKukMrYeg \t 0.308230511122904\n",
      "10 AEiNkWY-4ggToDeMTd8l1w \t 0.2990933593101938\n"
     ]
    }
   ],
   "source": [
    "testquery=\"best bbq\"\n",
    "scoring=\"cosine_score\"\n",
    "doc_metric,idfarray=score_documents(testquery,scoring)\n",
    "composite_score=cosine_score()\n",
    "printresults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking with BM25\n",
    "\n",
    "BM-25 Introduction [https://en.wikipedia.org/wiki/Okapi_BM25](https://en.wikipedia.org/wiki/Okapi_BM25) \n",
    "I have chosen k_1 = 1.2 and b = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_score():\n",
    "    score=dict()\n",
    "    k=1.2\n",
    "    b=0.75\n",
    "    for x in list(doc_metric): #iterate over all docs with non zero tf\n",
    "        scr=0\n",
    "        for i in range(len(doc_metric[x][0])):\n",
    "            scr+=idfarray[i]*((doc_metric[x][0][i]*(1+k))/(doc_metric[x][0][i]+k*(1-b+b*(doc_metric[x][1])/avgdl)))\n",
    "        score[x]=scr\n",
    "    #Calculate composite Sum of tf*idf score for each term\n",
    "    return score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query best bbq\n",
      "Rank Document ID \t\t Score \n",
      "\n",
      "1 x5esEK6J9XkA_vbvVbG8Gg \t 9.721870551819865\n",
      "2 xpm6TgDiHaQdEDlErFsqvQ \t 9.415118348525638\n",
      "3 4WTG1-9mw8YHEyaTu8dQww \t 8.959421822569597\n",
      "4 e5INq6DAZn2zMHicKQl07Q \t 8.592495681676898\n",
      "5 GASAd_gPBY_eWIL9XJwuNA \t 7.976652147350818\n",
      "6 P31kXP4oan6ZQm69TN6tIA \t 7.880072215237718\n",
      "7 8p-KEtrrTmLv-o1mKpUy1A \t 7.619530382843498\n",
      "8 HzNxErSCQ2FYfPCbyfHrSQ \t 7.436157918661526\n",
      "9 -RApX_RMzJLnpommDpQfKQ \t 7.377703314704793\n",
      "10 1tJ_iJX_KZ3zM_9_GRaGTg \t 7.193190780305126\n"
     ]
    }
   ],
   "source": [
    "testquery=\"best bbq\"\n",
    "scoring=\"bm25_score\"\n",
    "doc_metric,idfarray=score_documents(testquery,scoring)\n",
    "composite_score=bm25_score()\n",
    "printresults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion on the three methods:\n",
    "\n",
    "TF*IDF method:\n",
    "It is the simplest method to implement. However the top match for \"best bbq\" was just a review with \"bbq\" ie the rarer term, mentioned 6 times which is to be expected given that rarer terms have large IDF. There was no penalty for a longer document, top scoring doc is a 358 word review, it scores high only because of the rarer term mentioned more number of times, however this approch will skew results towards longer documents which will obviously mention a term more often. An approach with some doc normalisation will give better results.\n",
    "\n",
    "Cosine Method: \n",
    "This method takes care of document normalisation by using the doc_mag (the length of the doc to normalise the tf*idf score).\n",
    "Takes more computation because tf idf score is to be calculated for each term in each doc for normalisation.\n",
    "Cosine method's highest rated review for \"best bbq\" is just a short document with one term ie \"best\" even though \"best\" has lower idf than \"bbq\". Short doc length is over weighted/preferred in cosine approach. This approach severely penalises documents that are slightly longer and chooses one of the shortest docs as the best.\n",
    "\n",
    "BM-25: \n",
    "Seems to be the best method in terms of computation cycles and extracting most important document.\n",
    "Does not involve either the computation intensive ways of cosine doc normalisation (normalisation is with simple doc length) nor is it heavily skewed in favor of rarer terms.\n",
    "Best rated document for \"best bbq\" was a relatively short doc length of 110 words, with bbq freq=4 and best freq=1. This was the most pertinent and relevant doc for the given query."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
